What did you learn from the other student presentations on this week's topic?

* Will G
	* Evoloops, their major figures and concepts
		* Concepts repeated here since they are useful to look back on. From: [[Week 8.pdf]] 
			* Self-Replication
				* "Moore’s Definition: For all N ≥ 1, then there exists a time tN at which there exist at least N copies of C."
			* Self-Reproduction
				* Von Neumann’s Definition: A self-reproducer must have the capacity for inheritable mutation in addition to the ability to replicate itself.
	* More about Von Neumann problems
		1. How is it possible for a mechanistic system to produce something as or more complex than itself?  
		2. How is it possible for complexity to increase over several generations of reproduction (as in biology)?
		* First problem is solved through the concepts of self-examination and the discovery of genomes
		* Second problem requires novel mechanisms for inheritable variation
	* Core War was interesting and I wonder what makes one program win over another
	* Evolved Open Endedness is based around the idea that evolution must have begun with simple replicators - OE must have been gradually acquired
* Dominic Reilly
	* ALife traditionally relies on human intuition and laborious trial-and-error
	* To automate this
		* Option 1: Mathematically define interestingness, open-endedness, emergence
		* **Option 2: Ask a foundation model.**
	* Option 2 uses vision-language foundation models (FMs) which score rendered videos of simulation
		* Three search methods
			* Supervised target
			* Open-endedness
			* Illumination
		* Enables quantitative analysis
		* This could automate the search for not ALife but be extended to other domains
* Alex Brickley
	* AI Scientist
		* "automates entire research lifecycle"
		* Automated Peer Review
		* Novel contributions in diffusion models, transformers, and grokking.
		* $15 a paper
		* Runs in an open-ended loop
			* Using previous ideas and feedback to improve the next generation
		* **“When combined with the most capable LLMs, The AI Scientist is capable of producing papers judged by our automated reviewer as ‘Weak Accept’ at a top machine learning conference.”**
		* Issues
			* No vision capabilities
			* Unable to fix visual issues in the paper
			* Misleading results
			* Occasionally makes critical errors when writing and evaluating results
		* Even a small error can cause big consequences as a simple removal of a NOT can make something true in the AI Scientist's current and future papers

How does it connect to what you learned from your own work?
* EOE vs OEE can be applied to Artificial Society simulations and the way they are set-up
* Elements from different Artificial Societies could potentially go against one another in their own implementation of Core Wars
* FMs can be used as a way to evaluate Artificial Societies
	* Instead of having to rely on a metric like inefficiency as was done in [[What I learned from simulating 250 million elections]], carrying capacity in Sugarscape, segregation metric in [[Vi Hart + Nicky Case]], and money in [[Nicky Case (July 2017)]]
* When more AI Scientists become available they could communicate with one another and create their own Artificial Society and compare their to cohorts of academic humans 

What would you like to learn more about?
* What has are the results of Core War? What programs do best? What conclusions can be made from it?
* How can people integrate within Artificial Societies? How does that affect "real" societies and the artificial through that interaction? 
* What would be the result if we used the best LLMs today to create individual agents and put them into a confined space like Sugarscape (or maybe a 3D simulation so it's more like our world)? Would any signs of life or novelty findings result from their interaction with one another and the simulated environment?