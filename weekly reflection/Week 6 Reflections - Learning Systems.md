What did you learn from the other student presentations on this week's topic?

* I learned about NEAT, CPPNs, and HyperNEAT from William.
	* Some things I found neat from NEAT:
		* NEAT's approach of evolving both network weights and topology was unique as most others used fixed topology
		* Evo no devo (direct encoding)
	* CPPN:
		* can be evolved with NEAT
		* directly generate patterns from coordinate inputs
	* HyperNEAT:
		* NEAT used to generate CPPN
		* Uses heat maps
		* Scalable
* I learned what the term task-agnostic learning and further thoughts about meta-learning and the consequences of AI models from Alex.
	* I also got to look over a graphic that compared key features to approach which made me wonder who put it together and how they based the decisions on what approach had what feature. 
* I learned about ANML (A Neuromodulated Meta-Learning Algorithm) and evolving neural networks to solve different problems from Dominic.
	* I found the info that ANML learns to control when and where neurons activate and learn interesting

How does it connect to what you learned from your own work?

* Both Alex and Dominic's presentations talked about dealing with catastrophic forgetting, which was mentioned in the neural cellular automata with the lizard when forcibly resampling the single cell in the regeneration sample pool strategy [[Mordvinstev (2020)]] [[Week 6.pdf]]
* All the presentations dealt with ANNs in some capacity
* In terms of solving multiple problems as Dominic's second paper was focused on. Mordvinstev's lizard had to first learn how to grow for a certain number of steps (the first problem), then learn how to grow for steps beyond (second problem), regenerate (third), and rotate (fourth). [[Mordvinstev (2020)]] 
* Many of the other presentations sought to do what the early NCA [[Tavares (2015)]] wanted to do in the way of limitless emergence of complexity rather than [[Mordvinstev (2020)]] specific focus on a singular goal: creating resilient, regrowing emojis. 

What would you like to learn more about?

* I want to learn more about how to use ANNs to model human behavior and interpersonal dynamics. 
* The work of Mordvinstev and how he can create such interesting simulations and art.
* I also want to learn more about Mordvinstev, morphogenesis, and how to make all these cool evolving formations. I plan to try my hand at coding up some kind of NCA in Minecraft over the Summer and see how it goes.
* The usefulness of Graph Theory, I know there is a whole class on Graph Theory at Rose and I'd imagine a lot of material on it, my question would be how useful is the studying of it for ALife? I would imagine very much especially with these neural nets where it seems that there are a lot of graph-related implementations to be had.